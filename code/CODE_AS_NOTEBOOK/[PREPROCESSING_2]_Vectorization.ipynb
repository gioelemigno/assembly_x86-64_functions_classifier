{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features (Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is the extraction of features from dataset.\n",
    "Each json_file (asm_code) is trasformed as an array considering the following things:\n",
    "                                        (# = number of / occurences of)\n",
    "    > CFG ANALYSIS\n",
    "        - #loops\n",
    "        - #edges\n",
    "        - #nodes\n",
    "        - Cyclomatic Complexity (M)\n",
    "            M = E - N + 2P\n",
    "                E = # edge\n",
    "                N = # nodes\n",
    "                P = Connected Components (=1)\n",
    "        \n",
    "    >ASM ANALYSIS\n",
    "        In FOLDER_KEYWORD_ASM_ANALYSIS folder there are several file, each of them contains a type of keyword. (e.g. a type of instructions, a type of registers)\n",
    "        There are two different file extension:\n",
    "            .instr -> contains keywords that refered to a specific assembly type instruction (e.g. jump)\n",
    "            .reg    -> contains keywords of a type of registers \n",
    "        The differents is necessary since during the search, in a line can be only one instructions but two o more registers (>2 e.g. in a memory access)\n",
    "        Moreover there is also a difference in the vectorization of this two kinds of keyword, in the vectorization form of an asm_code there is the number of occurences of each keyword, and also a number that consideres the all categories. In the case of instructions, there is also the sum of all occurences of that type of instructions. In the case of registers instead, there is also the number of different register used (we are not interested about how many times the code uses register, but how much it uses, it is based on the concept that a complex program uses many registers)\n",
    "\n",
    "        It is also considered the number of memory access (NOTE: it is obtained by counting the occurrences of '[') and the number of lines of asm code\n",
    "\n",
    "        So, regarding to this part, the features are the following:\n",
    "            - #memory access\n",
    "            - #asm lines\n",
    "\n",
    "            For each file in FOLDER_KEYWORD_ASM_ANALYSIS: \n",
    "                if extension == .instr:\n",
    "                    for keyword in file:\n",
    "                        - #keyword\n",
    "                        - sum of all occurences\n",
    "                \n",
    "                else if extension == .reg:\n",
    "                    for keyword in file:\n",
    "                        - #keyword\n",
    "                        - number of keyword that occurences more than zero\n",
    "\n",
    "        \n",
    "NOTE:\n",
    "\n",
    "(1) The memory access are counted by the occurrences of '[' <br>\n",
    "(2) A Register and its subregisters are considered as different registers (i.e. %eax and %ax are considered as two different registers) <br>\n",
    "(3) The order of features regarding to ASM ANALYSIS, after #memory access and #asm lines, are in alphabetic order according to the name of keyword file \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_KEYWORD_ASM_ANALYSIS = './keyword_asm_analysis'\n",
    "\n",
    "#extensions of keyword files\n",
    "INSTR_EXT = '.instr'\n",
    "REG_EXT = '.reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_code_to_int = {'undefined': 0, 'encryption':1, 'string': 2,  'sort':3, 'math':4}\n",
    "\n",
    "semantic_code_to_string = ['undefined','encryption', 'string','sort', 'math']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_analysis(dict_graph):\n",
    "    '''\n",
    "    @dict_graph = graph as dictionary (output of json load)\n",
    "    @return an array with the following components:\n",
    "        > CFG ANALYSIS\n",
    "            - Number of loops\n",
    "            - Number of edges\n",
    "            - Number of nodes\n",
    "            - Cyclomatic Complexity:\n",
    "                M = E - N + 2P\n",
    "                    E = # edge\n",
    "                    N = # nodes\n",
    "                    P = Connected Components (=1)\n",
    "    '''\n",
    "    graph  = json_graph.adjacency_graph(dict_graph)\n",
    "\n",
    "    n_loops = len(nx.cycle_basis(graph.to_undirected()))\n",
    "    n_edges = graph.number_of_edges()\n",
    "    n_nodes = graph.number_of_nodes()\n",
    "    M = n_edges - n_nodes + 2\n",
    "\n",
    "    x = [n_loops, n_edges, n_nodes, M]\n",
    "    x_meaning = ['num_loops', 'num_edges', 'num_nodes', 'cyclomatic_complex']\n",
    "    \n",
    "    return x, x_meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_empty_dictionary(file_keys_path):\n",
    "    '''\n",
    "        Aux function to build empty dictionary used in asm analysis\n",
    "    '''\n",
    "    file_keys = open(file_keys_path, 'r', encoding='utf8')\n",
    "    lines =file_keys.readlines()\n",
    "    \n",
    "    dict_result = {}\n",
    "    for line in lines:\n",
    "        key = line.strip() \n",
    "        dict_result[key] = 0\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asm_decomposed_list(lista_asm):\n",
    "    '''\n",
    "        convert lista_asm to a list containing a dict for each instruction where key are: instruction, S, D\n",
    "        if an instruction doesn't have S or D, the value is empty = ''\n",
    "\n",
    "        @lista_asm = asm from json file\n",
    "        @return list of instruction\n",
    "    '''\n",
    "    result = []\n",
    "    raw_list = lista_asm[1:-2].strip().split(\"',\") # used \"',\" instead of a single comma to avoid split of a instrcution like: \"mov rbp, rsp\"\n",
    "\n",
    "    for instr in raw_list:\n",
    "        line = instr.strip()[1:] #remove \"'\" at the beginning\n",
    "\n",
    "        S=''\n",
    "        D=''\n",
    "        decomposed = line.split(\" \", 1) #INSTRUCTION - S,D\n",
    "        instr = decomposed[0].strip()\n",
    "        if(len(decomposed)>1): #search S and D\n",
    "            s_d = decomposed[1].split(',')\n",
    "            S = s_d[0].strip()\n",
    "            if(len(s_d) == 2):\n",
    "                D = s_d[1].strip()\n",
    "        dic = {}\n",
    "        dic['instruction']=instr\n",
    "        dic['S'] = S \n",
    "        dic['D'] = D\n",
    "        result.append(dic)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keyword(dict_keyword, string_to_parse, early_stop_search=True, are_register=False):\n",
    "    '''\n",
    "        search each keyword of dict inside string_to_parse (single asm instrcution) and increment the relative dictionary's entry\n",
    "\n",
    "        @dict_keyword = dict with keyword to search\n",
    "        @string_to_parse\n",
    "\n",
    "        @early_stop_search = (if true then it stop search at first keyword occurs)\n",
    "            // set to False during register search\n",
    "\n",
    "        @are_register = for each register, search for (e.g. ax):\n",
    "            ' ax' -> avoid count it in eax\n",
    "            '[ax' -> for memory access\n",
    "            'ax]'\n",
    "            ',ax'\n",
    "\n",
    "        @return:\n",
    "            False   no keyword found\n",
    "            True   keyword found\n",
    "\n",
    "        @side effect:   entries of dict corresponding to keyword found are incremented\n",
    "    '''\n",
    "    found_keyword = False\n",
    "    keys = dict_keyword.keys()\n",
    "    for key in keys:\n",
    "        if are_register:\n",
    "            if (' ' + key in string_to_parse):\n",
    "                found_keyword = True\n",
    "                dict_keyword[key] = dict_keyword[key] + 1 \n",
    "                if early_stop_search:\n",
    "                    return found_keyword\n",
    "\n",
    "            if ('[' + key in string_to_parse):\n",
    "                found_keyword = True\n",
    "                dict_keyword[key] = dict_keyword[key] + 1 \n",
    "                if early_stop_search:\n",
    "                    return found_keyword\n",
    "            '''\n",
    "            if (key + ']' in string_to_parse):\n",
    "                found_keyword = True\n",
    "                dict_keyword[key] = dict_keyword[key] + 1 \n",
    "                if early_stop_search:\n",
    "                    return found_keyword\n",
    "            '''\n",
    "\n",
    "        else:\n",
    "            if key in string_to_parse:\n",
    "                found_keyword = True\n",
    "                dict_keyword[key] = dict_keyword[key] + 1 \n",
    "                if early_stop_search:\n",
    "                    return found_keyword\n",
    "\n",
    "    return found_keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asm_analysis(asm_list_dict, keyword_folder):\n",
    "    '''\n",
    "        @asm_list_dict = asm code as list of dictionary in which each element is a line (NOT obtained directly by json load function). The keys are: instruction, S, D\n",
    "        if one operand is not in the line, then it is set to blank space.\n",
    "\n",
    "        @keyword_folder contains keywords to find in the assembly code. Each file containts a type of instruction divided according to their type. There are used two type of file extensions:\n",
    "        .instr = file contains keywords that are instrcution, so in each line only one of instrctrion can be there\n",
    "        .reg = file contains registers, instead in this case, in the src, dst part of asm code, can be two or more different registers (>2 for example in memory access) \n",
    "\n",
    "        @return array with component described above\n",
    "            NOTE: For each .instr file, the sum of all occurences of same categories are considered, also the singular occurence of a keyword.\n",
    "            (e.g. file.instr = {xor or and}, the features of the code will be:\n",
    "                #xor        (# = occurences of)\n",
    "                # or\n",
    "                # and\n",
    "                #xor + #or + #and)\n",
    "\n",
    "            NOTE_2: For .reg file instead and considered the number of different of register found in the asm code\n",
    "            (e.g. file.reg = {eax, ebx, ecx}\n",
    "                #eax\n",
    "                #ebx\n",
    "                #ecx\n",
    "                (#eax>0?1:0)+(#ebx>0?1:0)+(#ecx>0?1:0)\n",
    "            )\n",
    "    '''\n",
    "\n",
    "    #build a empty dictionary for each files\n",
    "    ordered_list_files = sorted(os.listdir(FOLDER_KEYWORD_ASM_ANALYSIS))\n",
    "    all_keywords ={} #contains a dictionary of dictionaries, the keys are filenames and the contents are dict with the occurences of each keys\n",
    "    '''\n",
    "    e.g.\n",
    "        all_keywords = {\n",
    "            'jump.instr': {'jmp':3, 'je':4},\n",
    "            'register.reg':{'eax':3, 'ecx':1}\n",
    "        }\n",
    "    '''\n",
    "    for filename in ordered_list_files:\n",
    "        all_keywords[filename] = to_empty_dictionary(os.path.join(FOLDER_KEYWORD_ASM_ANALYSIS, filename))\n",
    "\n",
    "    MEMORY_ACCESS = '['\n",
    "    number_of_memory_access = 0\n",
    "\n",
    "    number_of_asm_lines = len(asm_list_dict)\n",
    "\n",
    "    for line_dict in asm_list_dict:\n",
    "        instruction_found = False #used to avoid search of other instruction\n",
    "\n",
    "        instruction = line_dict['instruction']\n",
    "        operands = \" \" + line_dict['S'] + \" \" + line_dict['D']\n",
    "\n",
    "        for keyword_type in all_keywords.keys():\n",
    "            if REG_EXT in keyword_type: #file contains keywords that refrers to registers\n",
    "                search_keyword(all_keywords[keyword_type], operands, early_stop_search=False, are_register=True)\n",
    "\n",
    "            if (INSTR_EXT in keyword_type) and (not instruction_found):\n",
    "                instruction_found = search_keyword(all_keywords[keyword_type], instruction)\n",
    "\n",
    "    \n",
    "        if MEMORY_ACCESS in operands:\n",
    "            number_of_memory_access = number_of_memory_access + 1\n",
    "\n",
    "    x=[]\n",
    "    x_meaning = []\n",
    "\n",
    "    x.append(number_of_memory_access)\n",
    "    x_meaning.append('memory_access')\n",
    "\n",
    "    x.append(number_of_asm_lines)\n",
    "    x_meaning.append('num_asm_lines')\n",
    "\n",
    "\n",
    "    for keyword_type in sorted(all_keywords):\n",
    "        name_type = keyword_type.split('.')[0]\n",
    "\n",
    "        if INSTR_EXT in keyword_type:\n",
    "            sum_occurences = 0\n",
    "            for keyword in all_keywords[keyword_type]:\n",
    "                occurences = all_keywords[keyword_type][keyword]\n",
    "                sum_occurences = sum_occurences + occurences\n",
    "                x.append(occurences)\n",
    "                x_meaning.append(keyword)\n",
    "            x.append(sum_occurences)\n",
    "            x_meaning.append(name_type)\n",
    "\n",
    "        elif REG_EXT in keyword_type:\n",
    "            sum_no_zero_occurences = 0\n",
    "            for keyword in all_keywords[keyword_type]:\n",
    "                occurences = all_keywords[keyword_type][keyword]\n",
    "                if occurences > 0:\n",
    "                    sum_no_zero_occurences = sum_no_zero_occurences + 1\n",
    "                x.append(occurences)\n",
    "                x_meaning.append(keyword)\n",
    "            x.append(sum_no_zero_occurences)\n",
    "            x_meaning.append(name_type)    \n",
    "\n",
    "    return x, x_meaning    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(json_file):\n",
    "    '''\n",
    "        return a dictionary:\n",
    "            ID = id of file (same of json file)\n",
    "            x = vectorization of file\n",
    "            y = {0,1,2,3,4} //ground truth\n",
    "                0: undefined (instance of test without label)\n",
    "                1: encryption\n",
    "                2: string\n",
    "                3: sort\n",
    "                4: math\n",
    "    '''\n",
    "    ID = json_file['id']\n",
    "    if 'semantic' in json_file.keys():\n",
    "        y = semantic_code_to_int[json_file['semantic']]\n",
    "    else:\n",
    "        y = semantic_code_to_int['undefined']   #blind_dataset\n",
    "\n",
    "    y_meaning = semantic_code_to_string\n",
    "\n",
    "    #start build x:\n",
    "    code_graph = json_file['cfg']\n",
    "    x_cfg, x_meaning_cfg = cfg_analysis(code_graph)\n",
    "\n",
    "    asm_file = json_file['lista_asm']\n",
    "    asm_code_list_dict = asm_decomposed_list(asm_file)\n",
    "    x_asm, x_meaning_asm = asm_analysis(asm_code_list_dict, FOLDER_KEYWORD_ASM_ANALYSIS)\n",
    "\n",
    "    x = x_cfg + x_asm\n",
    "    x_meaning = x_meaning_cfg + x_meaning_asm\n",
    "\n",
    "\n",
    "    return ID, x_meaning, y_meaning, x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT DATASET TO JSON FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exported file is a singular json file that contains the following keys: <br>\n",
    "- x_meaning = meaning of each features <br>\n",
    "- y_meaning = mapping index to semantic <br>\n",
    "- dataset = dataset: <br>\n",
    ">For each sample:<br>\n",
    "    |--------ID -> ID from dataset provided <br>\n",
    "    |--------x -> features <br>\n",
    "    |--------y -> class (index) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataset_to_file(_path, _x_meaning, _y_meaning, _IDs, _X, _Y,):\n",
    "    dataset_out_dict={} #dataset in dict format\n",
    "    dataset_out_dict['x_meaning'] = _x_meaning\n",
    "    dataset_out_dict['y_meaning'] = _y_meaning\n",
    "\n",
    "    dataset_out_dict['dataset']=[]\n",
    "\n",
    "    if (len(_IDs) != len(_X)) or (len(_X) != len(_Y)):\n",
    "        print(\"Error, wrong dimensions\")\n",
    "        return None\n",
    "  \n",
    "    for idx in range(len(_X)):\n",
    "        new_element = {}\n",
    "        new_element['ID'] = _IDs[idx]\n",
    "        new_element['x'] = _X[idx]\n",
    "        new_element['y'] = _Y[idx]\n",
    "\n",
    "        dataset_out_dict['dataset'].append(new_element)\n",
    "\n",
    "    #write dataset_out_dict on json file\n",
    "    file_dataset_out = open(_path, 'w')\n",
    "    json.dump(dataset_out_dict, file_dataset_out)\n",
    "    file_dataset_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_dataset_and_export(_path_src, _path_dst):\n",
    "    dataset_file = open(_path_src, 'r')\n",
    "    dataset_file_lines = dataset_file.readlines()\n",
    "\n",
    "    json_files = []\n",
    "    for line in dataset_file_lines:\n",
    "        json_files.append(json.loads(line.strip()))\n",
    "\n",
    "    #write first keys\n",
    "    ID, _x_meaning, _y_meaning, x, y  = vectorization(json_files[0])\n",
    "    x_meaning = _x_meaning\n",
    "    y_meaning = _y_meaning\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    IDs = []\n",
    "    for json_file in json_files:\n",
    "        ID, _x_meaning, _y_meaning, x, y  = vectorization(json_file)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        IDs.append(ID) \n",
    "\n",
    "    export_dataset_to_file(_path_dst, x_meaning, y_meaning, IDs, X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DATASET_DUP = './dataset/original/DUP/dataset.json'\n",
    "DST_DATASET_DUP =  './dataset/vectorizated/DUP/dataset.json'\n",
    "\n",
    "SRC_DATASET_DUP_BLIND = './dataset/original/DUP/blindtest.json'\n",
    "DST_DATASET_DUP_BLIND = './dataset/vectorizated/DUP/blindtest.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_dataset_and_export(SRC_DATASET_DUP,DST_DATASET_DUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_dataset_and_export(SRC_DATASET_DUP_BLIND,DST_DATASET_DUP_BLIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "757"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "f = open(SRC_DATASET_DUP_BLIND, 'r')\n",
    "lines = f.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with NO duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DATASET_NO_DUP = './dataset/original/NO_DUP/noduplicatedataset.json'\n",
    "DST_DATASET_NO_DUP =  './dataset/vectorizated/NO_DUP/noduplicatedataset.json'\n",
    "\n",
    "SRC_DATASET_NO_DUP_BLIND = './dataset/original/NO_DUP/nodupblindtest.json'\n",
    "DST_DATASET_NO_DUP_BLIND = './dataset/vectorizated/NO_DUP/nodupblindtest.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_dataset_and_export(SRC_DATASET_NO_DUP,DST_DATASET_NO_DUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_dataset_and_export(SRC_DATASET_NO_DUP_BLIND,DST_DATASET_NO_DUP_BLIND)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}